## 1. Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots [link](https://arxiv.org/abs/2307.08715)

## 2. Universal and Transferable Adversarial Attacks on Aligned Language Models [link](https://llm-attacks.org/)

An interesting LLM jailbreak attack via gradient-based optimization. Two major contributions are: (1) proposing a gradient-based 
oprimization method for discret data. (2) defining the optimization problem of LLM jailbreak. Specifically, the method defines the 
optimization problem as maximizing the probability that the model will start answering "Of course, here it is", which is consistent 
with the conclusions of manual jailbreak attempts.
